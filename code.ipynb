{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63b2cf0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T10:42:09.082376Z",
     "iopub.status.busy": "2025-04-17T10:42:09.081620Z",
     "iopub.status.idle": "2025-04-17T10:42:13.641826Z",
     "shell.execute_reply": "2025-04-17T10:42:13.640936Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmetagpt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlibs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meditor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Editor\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Create the race data structure (same as before)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m race_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m----> 8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_boats\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mint\u001b[39m(\u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid_athlete\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique()),\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregatta_number\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mint\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregatta_number\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]),\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeographic_bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     11\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     12\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misna(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin()) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude_strapolated\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin()),\n\u001b[1;32m     13\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misna(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude_strapolated\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax())\n\u001b[1;32m     14\u001b[0m             },\n\u001b[1;32m     15\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     16\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misna(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin()) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude_strapolated\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin()),\n\u001b[1;32m     17\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misna(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude_strapolated\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax())\n\u001b[1;32m     18\u001b[0m             }\n\u001b[1;32m     19\u001b[0m         },\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_range\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     21\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m: df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_time\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS.\u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     22\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m: df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_time\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS.\u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m         }\n\u001b[1;32m     24\u001b[0m     },\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboats\u001b[39m\u001b[38;5;124m\"\u001b[39m: {}\n\u001b[1;32m     26\u001b[0m }\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Process data for each boat\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m boat_id \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid_athlete\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from metagpt.tools.libs.editor import Editor\n",
    "\n",
    "# Create the race data structure (same as before)\n",
    "race_data = {\n",
    "    \"metadata\": {\n",
    "        \"total_boats\": int(df['id_athlete'].nunique()),\n",
    "        \"regatta_number\": int(df['regatta_number'].iloc[0]),\n",
    "        \"geographic_bounds\": {\n",
    "            \"latitude\": {\n",
    "                \"min\": float(df['latitude'].min()) if not pd.isna(df['latitude'].min()) else float(df['latitude_strapolated'].min()),\n",
    "                \"max\": float(df['latitude'].max()) if not pd.isna(df['latitude'].max()) else float(df['latitude_strapolated'].max())\n",
    "            },\n",
    "            \"longitude\": {\n",
    "                \"min\": float(df['longitude'].min()) if not pd.isna(df['longitude'].min()) else float(df['longitude_strapolated'].min()),\n",
    "                \"max\": float(df['longitude'].max()) if not pd.isna(df['longitude'].max()) else float(df['longitude_strapolated'].max())\n",
    "            }\n",
    "        },\n",
    "        \"time_range\": {\n",
    "            \"start\": df['sample_time'].min().strftime('%Y-%m-%dT%H:%M:%S.%fZ'),\n",
    "            \"end\": df['sample_time'].max().strftime('%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "        }\n",
    "    },\n",
    "    \"boats\": {}\n",
    "}\n",
    "\n",
    "# Process data for each boat\n",
    "for boat_id in df['id_athlete'].unique():\n",
    "    boat_df = df[df['id_athlete'] == boat_id].copy()\n",
    "    \n",
    "    # Sort by timestamp to ensure proper ordering\n",
    "    boat_df = boat_df.sort_values('sample_time')\n",
    "    \n",
    "    # Use strapolated coordinates if original ones are missing\n",
    "    coordinates = []\n",
    "    for _, row in boat_df.iterrows():\n",
    "        lat = row['latitude'] if not pd.isna(row['latitude']) else row['latitude_strapolated']\n",
    "        lon = row['longitude'] if not pd.isna(row['longitude']) else row['longitude_strapolated']\n",
    "        coordinates.append([float(lon), float(lat)])  # GeoJSON format: [longitude, latitude]\n",
    "    \n",
    "    # Format timestamps as ISO format strings\n",
    "    timestamps = boat_df['sample_time'].dt.strftime('%Y-%m-%dT%H:%M:%S.%fZ').tolist()\n",
    "    \n",
    "    # Calculate statistics safely\n",
    "    avg_speed = float(boat_df['speed'].mean()) if not pd.isna(boat_df['speed'].mean()) else 0.0\n",
    "    max_speed = float(boat_df['speed'].max()) if not pd.isna(boat_df['speed'].max()) else 0.0\n",
    "    total_distance = float(boat_df['distance_from_start'].max()) if not pd.isna(boat_df['distance_from_start'].max()) else 0.0\n",
    "    \n",
    "    race_data[\"boats\"][str(int(boat_id))] = {\n",
    "        \"track\": {\n",
    "            \"type\": \"LineString\",\n",
    "            \"coordinates\": coordinates\n",
    "        },\n",
    "        \"properties\": {\n",
    "            \"timestamps\": timestamps,\n",
    "            \"speeds\": boat_df['speed'].fillna(0).astype(float).tolist(),\n",
    "            \"directions\": boat_df['direction'].fillna(0).astype(float).tolist(),\n",
    "            \"distance_from_start\": boat_df['distance_from_start'].fillna(0).astype(float).tolist(),\n",
    "            \"distance_to_finish\": boat_df['distance_to_finish'].fillna(0).astype(float).tolist()\n",
    "        },\n",
    "        \"metadata\": {\n",
    "            \"avg_speed\": avg_speed,\n",
    "            \"max_speed\": max_speed,\n",
    "            \"total_distance\": total_distance\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Initialize editor\n",
    "editor = Editor()\n",
    "\n",
    "# Convert race_data to JSON string with proper indentation\n",
    "json_content = json.dumps(race_data, indent=2)\n",
    "\n",
    "# Save using the Editor tool\n",
    "await editor.write('data/race_data.json', json_content)\n",
    "\n",
    "print(\"Race data has been successfully exported to race_data.json\")\n",
    "print(f\"Total boats processed: {len(race_data['boats'])}\")\n",
    "print(f\"Geographic bounds: {race_data['metadata']['geographic_bounds']}\")\n",
    "print(f\"Time range: {race_data['metadata']['time_range']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f032093d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T10:42:44.620214Z",
     "iopub.status.busy": "2025-04-17T10:42:44.619388Z",
     "iopub.status.idle": "2025-04-17T10:42:45.227182Z",
     "shell.execute_reply": "2025-04-17T10:42:45.225863Z"
    }
   },
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m             clean_lines\u001b[38;5;241m.\u001b[39mappend(content)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Create DataFrame from cleaned data\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mStringIO\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_lines\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelim_whitespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Handle multiple spaces\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msample_time\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msample_time_strapolated\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Create race data structure\u001b[39;00m\n\u001b[1;32m     32\u001b[0m race_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_boats\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mint\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid_athlete\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique()),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboats\u001b[39m\u001b[38;5;124m\"\u001b[39m: {}\n\u001b[1;32m     52\u001b[0m }\n",
      "File \u001b[0;32m/opt/conda/envs/mgx-chat/lib/python3.10/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/mgx-chat/lib/python3.10/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/envs/mgx-chat/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/mgx-chat/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1723\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1720\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1725\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/mgx-chat/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32mparsers.pyx:586\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "from metagpt.tools.libs.editor import Editor\n",
    "\n",
    "# Initialize editor\n",
    "editor = Editor()\n",
    "\n",
    "# Read the GPS data file\n",
    "file_content = await editor.read('uploads/gpsdata.csv')\n",
    "\n",
    "# Clean the raw content to get actual CSV data\n",
    "raw_lines = str(file_content).split('\\n')\n",
    "start_idx = next(i for i, line in enumerate(raw_lines) if '|' in line and 'id_meeting' in line)\n",
    "clean_lines = []\n",
    "for line in raw_lines[start_idx:]:\n",
    "    if '|' in line:\n",
    "        content = line.split('|')[1].strip()\n",
    "        if content and ('id_meeting' in content or content[0].isdigit()):\n",
    "            clean_lines.append(content)\n",
    "\n",
    "# Create DataFrame from cleaned data\n",
    "df = pd.read_csv(\n",
    "    StringIO('\\n'.join(clean_lines)),\n",
    "    delim_whitespace=True,  # Handle multiple spaces\n",
    "    parse_dates=['sample_time', 'sample_time_strapolated']\n",
    ")\n",
    "\n",
    "# Create race data structure\n",
    "race_data = {\n",
    "    \"metadata\": {\n",
    "        \"total_boats\": int(df['id_athlete'].nunique()),\n",
    "        \"regatta_number\": int(df['regatta_number'].iloc[0]),\n",
    "        \"geographic_bounds\": {\n",
    "            \"latitude\": {\n",
    "                \"min\": float(df['latitude'].min()) if not pd.isna(df['latitude'].min()) else float(df['latitude_strapolated'].min()),\n",
    "                \"max\": float(df['latitude'].max()) if not pd.isna(df['latitude'].max()) else float(df['latitude_strapolated'].max())\n",
    "            },\n",
    "            \"longitude\": {\n",
    "                \"min\": float(df['longitude'].min()) if not pd.isna(df['longitude'].min()) else float(df['longitude_strapolated'].min()),\n",
    "                \"max\": float(df['longitude'].max()) if not pd.isna(df['longitude'].max()) else float(df['longitude_strapolated'].max())\n",
    "            }\n",
    "        },\n",
    "        \"time_range\": {\n",
    "            \"start\": df['sample_time'].min().strftime('%Y-%m-%dT%H:%M:%S.%fZ'),\n",
    "            \"end\": df['sample_time'].max().strftime('%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "        }\n",
    "    },\n",
    "    \"boats\": {}\n",
    "}\n",
    "\n",
    "# Process data for each boat\n",
    "for boat_id in df['id_athlete'].unique():\n",
    "    boat_df = df[df['id_athlete'] == boat_id].copy()\n",
    "    boat_df = boat_df.sort_values('sample_time')\n",
    "    \n",
    "    # Use strapolated coordinates if original ones are missing\n",
    "    coordinates = []\n",
    "    for _, row in boat_df.iterrows():\n",
    "        lat = row['latitude'] if not pd.isna(row['latitude']) else row['latitude_strapolated']\n",
    "        lon = row['longitude'] if not pd.isna(row['longitude']) else row['longitude_strapolated']\n",
    "        if not pd.isna(lat) and not pd.isna(lon):\n",
    "            coordinates.append([float(lon), float(lat)])  # GeoJSON format: [longitude, latitude]\n",
    "    \n",
    "    # Ensure all numeric values are properly handled\n",
    "    avg_speed = float(boat_df['speed'].mean()) if not pd.isna(boat_df['speed'].mean()) else 0.0\n",
    "    max_speed = float(boat_df['speed'].max()) if not pd.isna(boat_df['speed'].max()) else 0.0\n",
    "    total_distance = float(boat_df['distance_from_start'].max()) if not pd.isna(boat_df['distance_from_start'].max()) else 0.0\n",
    "    \n",
    "    race_data[\"boats\"][str(int(boat_id))] = {\n",
    "        \"track\": {\n",
    "            \"type\": \"LineString\",\n",
    "            \"coordinates\": coordinates\n",
    "        },\n",
    "        \"properties\": {\n",
    "            \"timestamps\": boat_df['sample_time'].dt.strftime('%Y-%m-%dT%H:%M:%S.%fZ').tolist(),\n",
    "            \"speeds\": boat_df['speed'].fillna(0).astype(float).tolist(),\n",
    "            \"directions\": boat_df['direction'].fillna(0).astype(float).tolist(),\n",
    "            \"distance_from_start\": boat_df['distance_from_start'].fillna(0).astype(float).tolist(),\n",
    "            \"distance_to_finish\": boat_df['distance_to_finish'].fillna(0).astype(float).tolist()\n",
    "        },\n",
    "        \"metadata\": {\n",
    "            \"avg_speed\": avg_speed,\n",
    "            \"max_speed\": max_speed,\n",
    "            \"total_distance\": total_distance\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Convert to JSON and save using Editor\n",
    "json_content = json.dumps(race_data, indent=2)\n",
    "await editor.write('data/race_data.json', json_content)\n",
    "\n",
    "print(\"Race data has been successfully exported to race_data.json\")\n",
    "print(f\"Total boats processed: {len(race_data['boats'])}\")\n",
    "print(f\"Geographic bounds: {race_data['metadata']['geographic_bounds']}\")\n",
    "print(f\"Time range: {race_data['metadata']['time_range']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0279b1b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T10:43:23.835947Z",
     "iopub.status.busy": "2025-04-17T10:43:23.835014Z",
     "iopub.status.idle": "2025-04-17T10:43:23.915790Z",
     "shell.execute_reply": "2025-04-17T10:43:23.914383Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found header and data\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '5030adba-5014-4e7e-a0d1-344cd501d81b'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 49\u001b[0m\n\u001b[1;32m     43\u001b[0m     df[col] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[col])\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Prepare race data structure\u001b[39;00m\n\u001b[1;32m     46\u001b[0m race_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_boats\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mint\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid_athlete\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique()),\n\u001b[0;32m---> 49\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregatta_number\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mint\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregatta_number\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]),\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeographic_bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     51\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     52\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misna(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin()) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude_strapolated\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin()),\n\u001b[1;32m     53\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misna(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude_strapolated\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax())\n\u001b[1;32m     54\u001b[0m             },\n\u001b[1;32m     55\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     56\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misna(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin()) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude_strapolated\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin()),\n\u001b[1;32m     57\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misna(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude_strapolated\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax())\n\u001b[1;32m     58\u001b[0m             }\n\u001b[1;32m     59\u001b[0m         },\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_range\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     61\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m: df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_time\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS.\u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     62\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m: df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_time\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS.\u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     63\u001b[0m         }\n\u001b[1;32m     64\u001b[0m     },\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboats\u001b[39m\u001b[38;5;124m\"\u001b[39m: {}\n\u001b[1;32m     66\u001b[0m }\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Process each boat's data\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m boat_id \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid_athlete\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique():\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '5030adba-5014-4e7e-a0d1-344cd501d81b'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "from metagpt.tools.libs.editor import Editor\n",
    "\n",
    "# Initialize editor and read file\n",
    "editor = Editor()\n",
    "file_content = await editor.read('uploads/gpsdata.csv')\n",
    "\n",
    "# Debug: Print first few lines to understand structure\n",
    "raw_lines = str(file_content).split('\\\\n')\n",
    "header = None\n",
    "data_lines = []\n",
    "\n",
    "# Process lines to extract clean CSV data\n",
    "for line in raw_lines:\n",
    "    if '|' in line:\n",
    "        # Remove line numbers and get content after pipe\n",
    "        parts = line.split('|')\n",
    "        if len(parts) > 1:\n",
    "            content = parts[1].strip()\n",
    "            if 'id_meeting' in content:  # This is the header row\n",
    "                header = content\n",
    "            elif content and content[0].isdigit():  # This is a data row\n",
    "                data_lines.append(content)\n",
    "\n",
    "# Check if we have valid data\n",
    "if header and data_lines:\n",
    "    print(\"Found header and data\")\n",
    "    \n",
    "    # Create DataFrame with careful parsing\n",
    "    df = pd.read_csv(\n",
    "        StringIO(header + '\\n' + '\\n'.join(data_lines)),\n",
    "        sep=r'\\s+',  # Handle multiple spaces between columns\n",
    "        engine='python'  # More flexible parsing engine\n",
    "    )\n",
    "    \n",
    "    # Convert date columns\n",
    "    time_cols = ['sample_time', 'sample_time_strapolated']\n",
    "    for col in time_cols:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "    \n",
    "    # Prepare race data structure\n",
    "    race_data = {\n",
    "        \"metadata\": {\n",
    "            \"total_boats\": int(df['id_athlete'].nunique()),\n",
    "            \"regatta_number\": int(df['regatta_number'].iloc[0]),\n",
    "            \"geographic_bounds\": {\n",
    "                \"latitude\": {\n",
    "                    \"min\": float(df['latitude'].min()) if not pd.isna(df['latitude'].min()) else float(df['latitude_strapolated'].min()),\n",
    "                    \"max\": float(df['latitude'].max()) if not pd.isna(df['latitude'].max()) else float(df['latitude_strapolated'].max())\n",
    "                },\n",
    "                \"longitude\": {\n",
    "                    \"min\": float(df['longitude'].min()) if not pd.isna(df['longitude'].min()) else float(df['longitude_strapolated'].min()),\n",
    "                    \"max\": float(df['longitude'].max()) if not pd.isna(df['longitude'].max()) else float(df['longitude_strapolated'].max())\n",
    "                }\n",
    "            },\n",
    "            \"time_range\": {\n",
    "                \"start\": df['sample_time'].min().strftime('%Y-%m-%dT%H:%M:%S.%fZ'),\n",
    "                \"end\": df['sample_time'].max().strftime('%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "            }\n",
    "        },\n",
    "        \"boats\": {}\n",
    "    }\n",
    "    \n",
    "    # Process each boat's data\n",
    "    for boat_id in df['id_athlete'].unique():\n",
    "        boat_df = df[df['id_athlete'] == boat_id].copy()\n",
    "        boat_df = boat_df.sort_values('sample_time')\n",
    "        \n",
    "        # Process coordinates\n",
    "        coordinates = []\n",
    "        for _, row in boat_df.iterrows():\n",
    "            lat = row['latitude'] if not pd.isna(row['latitude']) else row['latitude_strapolated']\n",
    "            lon = row['longitude'] if not pd.isna(row['longitude']) else row['longitude_strapolated']\n",
    "            if not pd.isna(lat) and not pd.isna(lon):\n",
    "                coordinates.append([float(lon), float(lat)])\n",
    "        \n",
    "        # Calculate statistics safely\n",
    "        stats = {\n",
    "            \"avg_speed\": float(boat_df['speed'].mean()) if not pd.isna(boat_df['speed'].mean()) else 0.0,\n",
    "            \"max_speed\": float(boat_df['speed'].max()) if not pd.isna(boat_df['speed'].max()) else 0.0,\n",
    "            \"total_distance\": float(boat_df['distance_from_start'].max()) if not pd.isna(boat_df['distance_from_start'].max()) else 0.0\n",
    "        }\n",
    "        \n",
    "        race_data[\"boats\"][str(int(boat_id))] = {\n",
    "            \"track\": {\n",
    "                \"type\": \"LineString\",\n",
    "                \"coordinates\": coordinates\n",
    "            },\n",
    "            \"properties\": {\n",
    "                \"timestamps\": boat_df['sample_time'].dt.strftime('%Y-%m-%dT%H:%M:%S.%fZ').tolist(),\n",
    "                \"speeds\": boat_df['speed'].fillna(0).astype(float).tolist(),\n",
    "                \"directions\": boat_df['direction'].fillna(0).astype(float).tolist(),\n",
    "                \"distance_from_start\": boat_df['distance_from_start'].fillna(0).astype(float).tolist(),\n",
    "                \"distance_to_finish\": boat_df['distance_to_finish'].fillna(0).astype(float).tolist()\n",
    "            },\n",
    "            \"metadata\": stats\n",
    "        }\n",
    "    \n",
    "    # Save to JSON file using Editor\n",
    "    json_content = json.dumps(race_data, indent=2)\n",
    "    await editor.write('data/race_data.json', json_content)\n",
    "    \n",
    "    print(\"Race data has been successfully exported to race_data.json\")\n",
    "    print(f\"Total boats processed: {len(race_data['boats'])}\")\n",
    "    print(f\"Geographic bounds: {race_data['metadata']['geographic_bounds']}\")\n",
    "    print(f\"Time range: {race_data['metadata']['time_range']}\")\n",
    "else:\n",
    "    print(\"Error: Could not find valid header or data in the file content\")\n",
    "    print(\"First few lines of file content:\")\n",
    "    print('\\n'.join(raw_lines[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43cab5c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T10:43:53.562277Z",
     "iopub.status.busy": "2025-04-17T10:43:53.561634Z",
     "iopub.status.idle": "2025-04-17T10:43:53.648315Z",
     "shell.execute_reply": "2025-04-17T10:43:53.646804Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object str can't be used in 'await' expression",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 77\u001b[0m\n\u001b[1;32m     51\u001b[0m     race_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboats\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;28mstr\u001b[39m(boat_id)] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrack\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     53\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLineString\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: stats\n\u001b[1;32m     74\u001b[0m     }\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Save processed data using Editor\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m editor\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/race_data.json\u001b[39m\u001b[38;5;124m'\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(race_data, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRace data has been successfully processed and saved\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal boats processed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(race_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboats\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: object str can't be used in 'await' expression"
     ]
    }
   ],
   "source": [
    "# Process the data into properly structured JSON\n",
    "race_data = {\n",
    "    \"metadata\": {\n",
    "        \"total_boats\": len(df['id_athlete'].unique()),\n",
    "        \"regatta_number\": df['regatta_number'].iloc[0],  # Keep as is, no int casting\n",
    "        \"geographic_bounds\": {\n",
    "            \"latitude\": {\n",
    "                \"min\": float(df['latitude_strapolated'].min()),\n",
    "                \"max\": float(df['latitude_strapolated'].max())\n",
    "            },\n",
    "            \"longitude\": {\n",
    "                \"min\": float(df['longitude'].min()),\n",
    "                \"max\": float(df['longitude'].max())\n",
    "            }\n",
    "        },\n",
    "        \"time_range\": {\n",
    "            \"start\": df['sample_time'].min().strftime('%Y-%m-%dT%H:%M:%S.%fZ'),\n",
    "            \"end\": df['sample_time'].max().strftime('%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "        }\n",
    "    },\n",
    "    \"boats\": {}\n",
    "}\n",
    "\n",
    "# Process each boat's data\n",
    "for boat_id in df['id_athlete'].unique():\n",
    "    boat_df = df[df['id_athlete'] == boat_id].copy()\n",
    "    boat_df = boat_df.sort_values('sample_time')\n",
    "    \n",
    "    # Use strapolated coordinates where original ones are missing\n",
    "    boat_df['final_latitude'] = boat_df['latitude'].fillna(boat_df['latitude_strapolated'])\n",
    "    boat_df['final_longitude'] = boat_df['longitude'].fillna(boat_df['longitude_strapolated'])\n",
    "    \n",
    "    # Ensure numeric types for calculations\n",
    "    numeric_cols = ['speed', 'direction', 'distance_from_start', 'distance_to_finish']\n",
    "    for col in numeric_cols:\n",
    "        boat_df[col] = pd.to_numeric(boat_df[col], errors='coerce').fillna(0)\n",
    "    \n",
    "    # Create track coordinates\n",
    "    coordinates = [[float(lon), float(lat)] \n",
    "                  for lon, lat in zip(boat_df['final_longitude'], boat_df['final_latitude'])\n",
    "                  if not (pd.isna(lon) or pd.isna(lat))]\n",
    "    \n",
    "    # Calculate statistics\n",
    "    stats = {\n",
    "        \"avg_speed\": float(boat_df['speed'].mean()),\n",
    "        \"max_speed\": float(boat_df['speed'].max()),\n",
    "        \"total_distance\": float(boat_df['distance_from_start'].max())\n",
    "    }\n",
    "    \n",
    "    # Store boat data\n",
    "    race_data[\"boats\"][str(boat_id)] = {\n",
    "        \"track\": {\n",
    "            \"type\": \"LineString\",\n",
    "            \"coordinates\": coordinates\n",
    "        },\n",
    "        \"timepoints\": [\n",
    "            {\n",
    "                \"timestamp\": ts.strftime('%Y-%m-%dT%H:%M:%S.%fZ'),\n",
    "                \"position\": [float(lon), float(lat)],\n",
    "                \"speed\": float(spd),\n",
    "                \"direction\": float(dir),\n",
    "                \"distance_from_start\": float(dist)\n",
    "            }\n",
    "            for ts, lon, lat, spd, dir, dist in zip(\n",
    "                boat_df['sample_time'],\n",
    "                boat_df['final_longitude'],\n",
    "                boat_df['final_latitude'],\n",
    "                boat_df['speed'],\n",
    "                boat_df['direction'],\n",
    "                boat_df['distance_from_start']\n",
    "            )\n",
    "        ],\n",
    "        \"metadata\": stats\n",
    "    }\n",
    "\n",
    "# Save processed data using Editor\n",
    "await editor.write('data/race_data.json', json.dumps(race_data, indent=2))\n",
    "print(\"Race data has been successfully processed and saved\")\n",
    "print(f\"Total boats processed: {len(race_data['boats'])}\")\n",
    "print(f\"Time range: {race_data['metadata']['time_range']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccf2839d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T10:44:15.239455Z",
     "iopub.status.busy": "2025-04-17T10:44:15.238741Z",
     "iopub.status.idle": "2025-04-17T10:44:15.282551Z",
     "shell.execute_reply": "2025-04-17T10:44:15.281556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Race data has been successfully processed and saved\n",
      "Total boats processed: 5\n",
      "Time range: {'start': '1970-01-01T00:00:00.000000Z', 'end': '1970-01-01T00:00:00.000000Z'}\n"
     ]
    }
   ],
   "source": [
    "# Process the data into properly structured JSON\n",
    "race_data = {\n",
    "    \"metadata\": {\n",
    "        \"total_boats\": len(df['id_athlete'].unique()),\n",
    "        \"regatta_number\": df['regatta_number'].iloc[0],  # Keep as is, no int casting\n",
    "        \"geographic_bounds\": {\n",
    "            \"latitude\": {\n",
    "                \"min\": float(df['latitude_strapolated'].min()),\n",
    "                \"max\": float(df['latitude_strapolated'].max())\n",
    "            },\n",
    "            \"longitude\": {\n",
    "                \"min\": float(df['longitude'].min()),\n",
    "                \"max\": float(df['longitude'].max())\n",
    "            }\n",
    "        },\n",
    "        \"time_range\": {\n",
    "            \"start\": df['sample_time'].min().strftime('%Y-%m-%dT%H:%M:%S.%fZ'),\n",
    "            \"end\": df['sample_time'].max().strftime('%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "        }\n",
    "    },\n",
    "    \"boats\": {}\n",
    "}\n",
    "\n",
    "# Process each boat's data\n",
    "for boat_id in df['id_athlete'].unique():\n",
    "    boat_df = df[df['id_athlete'] == boat_id].copy()\n",
    "    boat_df = boat_df.sort_values('sample_time')\n",
    "    \n",
    "    # Use strapolated coordinates where original ones are missing\n",
    "    boat_df['final_latitude'] = boat_df['latitude'].fillna(boat_df['latitude_strapolated'])\n",
    "    boat_df['final_longitude'] = boat_df['longitude'].fillna(boat_df['longitude_strapolated'])\n",
    "    \n",
    "    # Ensure numeric types for calculations\n",
    "    numeric_cols = ['speed', 'direction', 'distance_from_start', 'distance_to_finish']\n",
    "    for col in numeric_cols:\n",
    "        boat_df[col] = pd.to_numeric(boat_df[col], errors='coerce').fillna(0)\n",
    "    \n",
    "    # Create track coordinates\n",
    "    coordinates = [[float(lon), float(lat)] \n",
    "                  for lon, lat in zip(boat_df['final_longitude'], boat_df['final_latitude'])\n",
    "                  if not (pd.isna(lon) or pd.isna(lat))]\n",
    "    \n",
    "    # Calculate statistics\n",
    "    stats = {\n",
    "        \"avg_speed\": float(boat_df['speed'].mean()),\n",
    "        \"max_speed\": float(boat_df['speed'].max()),\n",
    "        \"total_distance\": float(boat_df['distance_from_start'].max())\n",
    "    }\n",
    "    \n",
    "    # Store boat data\n",
    "    race_data[\"boats\"][str(boat_id)] = {\n",
    "        \"track\": {\n",
    "            \"type\": \"LineString\",\n",
    "            \"coordinates\": coordinates\n",
    "        },\n",
    "        \"timepoints\": [\n",
    "            {\n",
    "                \"timestamp\": ts.strftime('%Y-%m-%dT%H:%M:%S.%fZ'),\n",
    "                \"position\": [float(lon), float(lat)],\n",
    "                \"speed\": float(spd),\n",
    "                \"direction\": float(dir),\n",
    "                \"distance_from_start\": float(dist)\n",
    "            }\n",
    "            for ts, lon, lat, spd, dir, dist in zip(\n",
    "                boat_df['sample_time'],\n",
    "                boat_df['final_longitude'],\n",
    "                boat_df['final_latitude'],\n",
    "                boat_df['speed'],\n",
    "                boat_df['direction'],\n",
    "                boat_df['distance_from_start']\n",
    "            )\n",
    "        ],\n",
    "        \"metadata\": stats\n",
    "    }\n",
    "\n",
    "# Save processed data using Editor (without await since write is not async)\n",
    "editor.write('data/race_data.json', json.dumps(race_data, indent=2))\n",
    "print(\"Race data has been successfully processed and saved\")\n",
    "print(f\"Total boats processed: {len(race_data['boats'])}\")\n",
    "print(f\"Time range: {race_data['metadata']['time_range']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e3d87b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T10:44:49.608174Z",
     "iopub.status.busy": "2025-04-17T10:44:49.607694Z",
     "iopub.status.idle": "2025-04-17T10:44:49.664447Z",
     "shell.execute_reply": "2025-04-17T10:44:49.663613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of raw timestamp data:\n",
      "0  95  1   1970-01-01\n",
      "1  95  1   1970-01-01\n",
      "2  95  1   1970-01-01\n",
      "3  95  1   1970-01-01\n",
      "4  95  1   1970-01-01\n",
      "Name: sample_time, dtype: datetime64[ns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '5030adba-5014-4e7e-a0d1-344cd501d81b'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m      9\u001b[0m df_processed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboat_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_processed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregatta_number\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m df_processed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid_athlete\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Process data into correct JSON structure\u001b[39;00m\n\u001b[1;32m     12\u001b[0m race_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_boats\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(df_processed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboat_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()),\n\u001b[0;32m---> 15\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregatta_number\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf_processed\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mregatta_number\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeographic_bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     17\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     18\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(df_processed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude_strapolated\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin()),\n\u001b[1;32m     19\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(df_processed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude_strapolated\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax())\n\u001b[1;32m     20\u001b[0m             },\n\u001b[1;32m     21\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     22\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(df_processed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin()),\n\u001b[1;32m     23\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(df_processed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax())\n\u001b[1;32m     24\u001b[0m             }\n\u001b[1;32m     25\u001b[0m         },\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_range\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     27\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m: df_processed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_time\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     28\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m: df_processed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_time\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     29\u001b[0m         }\n\u001b[1;32m     30\u001b[0m     },\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboats\u001b[39m\u001b[38;5;124m\"\u001b[39m: {}\n\u001b[1;32m     32\u001b[0m }\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Process each boat's data\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m boat_id \u001b[38;5;129;01min\u001b[39;00m df_processed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboat_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique():\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '5030adba-5014-4e7e-a0d1-344cd501d81b'"
     ]
    }
   ],
   "source": [
    "# First check and fix the timestamp data\n",
    "print(\"Sample of raw timestamp data:\")\n",
    "print(df['sample_time'].head())\n",
    "\n",
    "# Convert timestamps correctly (they are already in datetime format from previous code)\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Fix boat ID processing - use regatta_number and id_athlete as composite key\n",
    "df_processed['boat_id'] = df_processed['regatta_number'].astype(str) + '_' + df_processed['id_athlete'].astype(str)\n",
    "\n",
    "# Process data into correct JSON structure\n",
    "race_data = {\n",
    "    \"metadata\": {\n",
    "        \"total_boats\": len(df_processed['boat_id'].unique()),\n",
    "        \"regatta_number\": int(df_processed['regatta_number'].iloc[0]),\n",
    "        \"geographic_bounds\": {\n",
    "            \"latitude\": {\n",
    "                \"min\": float(df_processed['latitude_strapolated'].min()),\n",
    "                \"max\": float(df_processed['latitude_strapolated'].max())\n",
    "            },\n",
    "            \"longitude\": {\n",
    "                \"min\": float(df_processed['longitude'].min()),\n",
    "                \"max\": float(df_processed['longitude'].max())\n",
    "            }\n",
    "        },\n",
    "        \"time_range\": {\n",
    "            \"start\": df_processed['sample_time'].min().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            \"end\": df_processed['sample_time'].max().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "    },\n",
    "    \"boats\": {}\n",
    "}\n",
    "\n",
    "# Process each boat's data\n",
    "for boat_id in df_processed['boat_id'].unique():\n",
    "    boat_df = df_processed[df_processed['boat_id'] == boat_id].copy()\n",
    "    boat_df = boat_df.sort_values('sample_time')\n",
    "    \n",
    "    # Ensure coordinates are properly handled\n",
    "    boat_df['final_latitude'] = boat_df['latitude'].fillna(boat_df['latitude_strapolated'])\n",
    "    boat_df['final_longitude'] = boat_df['longitude'].fillna(boat_df['longitude_strapolated'])\n",
    "    \n",
    "    # Process numeric data\n",
    "    numeric_cols = ['speed', 'direction', 'distance_from_start', 'distance_to_finish']\n",
    "    for col in numeric_cols:\n",
    "        boat_df[col] = pd.to_numeric(boat_df[col], errors='coerce').fillna(0)\n",
    "    \n",
    "    # Create track coordinates\n",
    "    coordinates = [[float(lon), float(lat)] \n",
    "                  for lon, lat in zip(boat_df['final_longitude'], boat_df['final_latitude'])\n",
    "                  if not (pd.isna(lon) or pd.isna(lat))]\n",
    "    \n",
    "    # Calculate boat statistics\n",
    "    stats = {\n",
    "        \"avg_speed\": float(boat_df['speed'].mean()),\n",
    "        \"max_speed\": float(boat_df['speed'].max()),\n",
    "        \"total_distance\": float(boat_df['distance_from_start'].max()),\n",
    "        \"duration_seconds\": (boat_df['sample_time'].max() - boat_df['sample_time'].min()).total_seconds()\n",
    "    }\n",
    "    \n",
    "    # Create timepoints with proper timestamp formatting\n",
    "    timepoints = []\n",
    "    for _, row in boat_df.iterrows():\n",
    "        timepoint = {\n",
    "            \"timestamp\": row['sample_time'].strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            \"position\": [float(row['final_longitude']), float(row['final_latitude'])],\n",
    "            \"speed\": float(row['speed']),\n",
    "            \"direction\": float(row['direction']),\n",
    "            \"distance_from_start\": float(row['distance_from_start'])\n",
    "        }\n",
    "        timepoints.append(timepoint)\n",
    "    \n",
    "    # Store boat data\n",
    "    race_data[\"boats\"][boat_id] = {\n",
    "        \"track\": {\n",
    "            \"type\": \"LineString\",\n",
    "            \"coordinates\": coordinates\n",
    "        },\n",
    "        \"timepoints\": timepoints,\n",
    "        \"metadata\": stats\n",
    "    }\n",
    "\n",
    "# Debug output\n",
    "print(\"\\nProcessed Data Summary:\")\n",
    "print(f\"Total boats: {len(race_data['boats'])}\")\n",
    "print(f\"Time range: {race_data['metadata']['time_range']}\")\n",
    "print(\"\\nSample timepoint data for first boat:\")\n",
    "first_boat_id = list(race_data[\"boats\"].keys())[0]\n",
    "print(json.dumps(race_data[\"boats\"][first_boat_id][\"timepoints\"][0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0640056",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T10:45:26.055877Z",
     "iopub.status.busy": "2025-04-17T10:45:26.055536Z",
     "iopub.status.idle": "2025-04-17T10:45:26.106545Z",
     "shell.execute_reply": "2025-04-17T10:45:26.105702Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data columns and types:\n",
      "id_meeting                          int64\n",
      "regatta_number                     object\n",
      "id_athlete                        float64\n",
      "id_position                       float64\n",
      "latitude                          float64\n",
      "longitude                         float64\n",
      "speed                              object\n",
      "direction                          object\n",
      "sample_time                datetime64[ns]\n",
      "distance_from_start               float64\n",
      "distance_to_finish                 object\n",
      "sample_time_strapolated    datetime64[ns]\n",
      "latitude_strapolated              float64\n",
      "longitude_strapolated              object\n",
      "dtype: object\n",
      "\n",
      "Sample of raw data:\n",
      "        id_meeting                        regatta_number  id_athlete  \\\n",
      "0 95 1           1  5030adba-5014-4e7e-a0d1-344cd501d81b   55.359260   \n",
      "1 95 1           1  8ceebd26-7e38-438e-824e-574c94f82392   55.359257   \n",
      "\n",
      "        id_position  latitude  longitude       speed     direction  \\\n",
      "0 95 1     9.206899       2.9       73.0  2023-08-31  07:29:25.000   \n",
      "1 95 1     9.206887       3.0       75.0  2023-08-31  07:29:24.000   \n",
      "\n",
      "       sample_time  distance_from_start distance_to_finish  \\\n",
      "0 95 1  1970-01-01         17372.938796         2023-08-31   \n",
      "1 95 1  1970-01-01         17372.938796         2023-08-31   \n",
      "\n",
      "       sample_time_strapolated  latitude_strapolated longitude_strapolated  \n",
      "0 95 1     2025-04-17 07:29:25             55.359260              9.206899  \n",
      "1 95 1     2025-04-17 07:29:24             55.359257              9.206887  \n",
      "Warning: Invalid timestamps detected, using original sample_time strings\n",
      "\n",
      "Processed Data Summary:\n",
      "Total boats: 5\n",
      "Regatta number: 5030adba-5014-4e7e-a0d1-344cd501d81b\n",
      "Time range: {'start': '1970-01-01 00:00:00', 'end': '1970-01-01 00:00:00'}\n",
      "\n",
      "Geographic bounds:\n",
      "{\n",
      "  \"latitude\": {\n",
      "    \"min\": 55.359251,\n",
      "    \"max\": 55.35926\n",
      "  },\n",
      "  \"longitude\": {\n",
      "    \"min\": 73.0,\n",
      "    \"max\": 76.0\n",
      "  }\n",
      "}\n",
      "\n",
      "Sample boat data:\n",
      "Number of timepoints: 1\n",
      "First timepoint:\n",
      "{\n",
      "  \"timestamp\": \"1970-01-01 00:00:00.000\",\n",
      "  \"position\": [\n",
      "    73.0,\n",
      "    2.9\n",
      "  ],\n",
      "  \"speed\": 0.0,\n",
      "  \"direction\": 0.0,\n",
      "  \"distance_from_start\": 17372.938796\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# First inspect and fix the data\n",
    "print(\"Data columns and types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nSample of raw data:\")\n",
    "print(df.head(2))\n",
    "\n",
    "# Create a processed copy of the dataframe\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Keep id_athlete as the boat identifier (no composite key needed)\n",
    "# Process data into JSON structure with proper type handling\n",
    "race_data = {\n",
    "    \"metadata\": {\n",
    "        \"total_boats\": len(df_processed['id_athlete'].unique()),\n",
    "        \"regatta_number\": str(df_processed['regatta_number'].iloc[0]),  # Keep as string\n",
    "        \"geographic_bounds\": {\n",
    "            \"latitude\": {\n",
    "                \"min\": float(df_processed['latitude_strapolated'].min()),\n",
    "                \"max\": float(df_processed['latitude_strapolated'].max())\n",
    "            },\n",
    "            \"longitude\": {\n",
    "                \"min\": float(df_processed['longitude'].min()),\n",
    "                \"max\": float(df_processed['longitude'].max())\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"boats\": {}\n",
    "}\n",
    "\n",
    "# Add time range only if timestamps are valid\n",
    "valid_time = df_processed['sample_time'].min().year > 1970\n",
    "if valid_time:\n",
    "    race_data[\"metadata\"][\"time_range\"] = {\n",
    "        \"start\": df_processed['sample_time'].min().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        \"end\": df_processed['sample_time'].max().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "else:\n",
    "    print(\"Warning: Invalid timestamps detected, using original sample_time strings\")\n",
    "    # Try to parse the original timestamp strings if available\n",
    "    if 'sample_time' in df.columns:\n",
    "        try:\n",
    "            df_processed['sample_time'] = pd.to_datetime(df['sample_time'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "            race_data[\"metadata\"][\"time_range\"] = {\n",
    "                \"start\": df_processed['sample_time'].min().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"end\": df_processed['sample_time'].max().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Could not parse timestamps: {str(e)}\")\n",
    "            race_data[\"metadata\"][\"time_range\"] = {\"start\": \"unknown\", \"end\": \"unknown\"}\n",
    "\n",
    "# Process each boat's data\n",
    "for boat_id in df_processed['id_athlete'].unique():\n",
    "    boat_df = df_processed[df_processed['id_athlete'] == boat_id].copy()\n",
    "    boat_df = boat_df.sort_values('sample_time')\n",
    "    \n",
    "    # Ensure coordinates are properly handled\n",
    "    boat_df['final_latitude'] = pd.to_numeric(\n",
    "        boat_df['latitude'].fillna(boat_df['latitude_strapolated']), \n",
    "        errors='coerce'\n",
    "    )\n",
    "    boat_df['final_longitude'] = pd.to_numeric(\n",
    "        boat_df['longitude'].fillna(boat_df['longitude_strapolated']), \n",
    "        errors='coerce'\n",
    "    )\n",
    "    \n",
    "    # Process numeric data safely\n",
    "    numeric_cols = ['speed', 'direction', 'distance_from_start', 'distance_to_finish']\n",
    "    for col in numeric_cols:\n",
    "        boat_df[col] = pd.to_numeric(boat_df[col], errors='coerce').fillna(0)\n",
    "    \n",
    "    # Create track coordinates (only where both lat and lon are valid)\n",
    "    coordinates = []\n",
    "    for idx, row in boat_df.iterrows():\n",
    "        if pd.notna(row['final_latitude']) and pd.notna(row['final_longitude']):\n",
    "            coordinates.append([\n",
    "                float(row['final_longitude']),\n",
    "                float(row['final_latitude'])\n",
    "            ])\n",
    "    \n",
    "    # Calculate boat statistics\n",
    "    stats = {\n",
    "        \"avg_speed\": float(boat_df['speed'].mean()),\n",
    "        \"max_speed\": float(boat_df['speed'].max()),\n",
    "        \"total_distance\": float(boat_df['distance_from_start'].max()),\n",
    "        \"points_count\": len(coordinates)\n",
    "    }\n",
    "    \n",
    "    # Create timepoints with proper validation\n",
    "    timepoints = []\n",
    "    for _, row in boat_df.iterrows():\n",
    "        if pd.notna(row['final_latitude']) and pd.notna(row['final_longitude']):\n",
    "            timepoint = {\n",
    "                \"timestamp\": row['sample_time'].strftime('%Y-%m-%d %H:%M:%S.%f')[:-3] \n",
    "                           if pd.notna(row['sample_time']) else \"unknown\",\n",
    "                \"position\": [float(row['final_longitude']), float(row['final_latitude'])],\n",
    "                \"speed\": float(row['speed']),\n",
    "                \"direction\": float(row['direction']),\n",
    "                \"distance_from_start\": float(row['distance_from_start'])\n",
    "            }\n",
    "            timepoints.append(timepoint)\n",
    "    \n",
    "    # Store boat data\n",
    "    race_data[\"boats\"][str(boat_id)] = {\n",
    "        \"track\": {\n",
    "            \"type\": \"LineString\",\n",
    "            \"coordinates\": coordinates\n",
    "        },\n",
    "        \"timepoints\": timepoints,\n",
    "        \"metadata\": stats\n",
    "    }\n",
    "\n",
    "# Debug output\n",
    "print(\"\\nProcessed Data Summary:\")\n",
    "print(f\"Total boats: {len(race_data['boats'])}\")\n",
    "print(f\"Regatta number: {race_data['metadata']['regatta_number']}\")\n",
    "print(\"Time range:\", race_data['metadata']['time_range'])\n",
    "print(\"\\nGeographic bounds:\")\n",
    "print(json.dumps(race_data['metadata']['geographic_bounds'], indent=2))\n",
    "\n",
    "if len(race_data['boats']) > 0:\n",
    "    first_boat = next(iter(race_data['boats'].values()))\n",
    "    print(\"\\nSample boat data:\")\n",
    "    print(f\"Number of timepoints: {len(first_boat['timepoints'])}\")\n",
    "    if first_boat['timepoints']:\n",
    "        print(\"First timepoint:\")\n",
    "        print(json.dumps(first_boat['timepoints'][0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "858c87b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T10:46:05.788433Z",
     "iopub.status.busy": "2025-04-17T10:46:05.787873Z",
     "iopub.status.idle": "2025-04-17T10:46:05.813347Z",
     "shell.execute_reply": "2025-04-17T10:46:05.811997Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed Data Summary:\n",
      "Total boats: 1\n",
      "Time range: {'start': '2025-04-17 07:29:21', 'end': '2025-04-17 07:29:25'}\n",
      "\n",
      "Sample timepoint:\n",
      "{\n",
      "  \"timestamp\": \"2025-04-17 07:29:21\",\n",
      "  \"position\": [\n",
      "    76.0,\n",
      "    55.359251\n",
      "  ],\n",
      "  \"speed\": 0.0,\n",
      "  \"direction\": 0.0,\n",
      "  \"distance_from_start\": 17372.938796\n",
      "}\n",
      "\n",
      "Coordinate bounds:\n",
      "{\n",
      "  \"latitude\": {\n",
      "    \"min\": 55.359251,\n",
      "    \"max\": 55.35926\n",
      "  },\n",
      "  \"longitude\": {\n",
      "    \"min\": 73.0,\n",
      "    \"max\": 76.0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Create a clean copy for processing\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Fix timestamp columns using sample_time_strapolated instead of sample_time\n",
    "df_clean['timestamp'] = pd.to_datetime(df_clean['sample_time_strapolated'])\n",
    "\n",
    "# Fix numeric columns\n",
    "df_clean['speed'] = pd.to_numeric(df_clean['speed'].str.split().str[0], errors='coerce')\n",
    "df_clean['direction'] = pd.to_numeric(df_clean['direction'].str.split().str[0], errors='coerce')\n",
    "df_clean['distance_from_start'] = pd.to_numeric(df_clean['distance_from_start'], errors='coerce')\n",
    "\n",
    "# Process coordinates (use strapolated when original is missing)\n",
    "df_clean['final_latitude'] = df_clean['latitude_strapolated']  # Use strapolated as they seem more valid\n",
    "df_clean['final_longitude'] = df_clean['longitude']  # Original longitude seems valid\n",
    "\n",
    "# Create the race data structure with corrected data\n",
    "race_data = {\n",
    "    \"metadata\": {\n",
    "        \"total_boats\": 1,  # We know it's one boat with multiple readings\n",
    "        \"regatta_number\": df_clean['regatta_number'].iloc[0],\n",
    "        \"geographic_bounds\": {\n",
    "            \"latitude\": {\n",
    "                \"min\": float(df_clean['final_latitude'].min()),\n",
    "                \"max\": float(df_clean['final_latitude'].max())\n",
    "            },\n",
    "            \"longitude\": {\n",
    "                \"min\": float(df_clean['final_longitude'].min()),\n",
    "                \"max\": float(df_clean['final_longitude'].max())\n",
    "            }\n",
    "        },\n",
    "        \"time_range\": {\n",
    "            \"start\": df_clean['timestamp'].min().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            \"end\": df_clean['timestamp'].max().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "    },\n",
    "    \"boats\": {}\n",
    "}\n",
    "\n",
    "# Process the single boat's data\n",
    "boat_id = df_clean['id_athlete'].iloc[0]  # Get the boat ID\n",
    "boat_df = df_clean.sort_values('timestamp')\n",
    "\n",
    "# Create track coordinates\n",
    "coordinates = [[float(lon), float(lat)] \n",
    "              for lon, lat in zip(boat_df['final_longitude'], boat_df['final_latitude'])\n",
    "              if not (pd.isna(lon) or pd.isna(lat))]\n",
    "\n",
    "# Calculate boat statistics\n",
    "stats = {\n",
    "    \"avg_speed\": float(boat_df['speed'].mean()),\n",
    "    \"max_speed\": float(boat_df['speed'].max()),\n",
    "    \"total_distance\": float(boat_df['distance_from_start'].max()),\n",
    "    \"points_count\": len(coordinates),\n",
    "    \"duration_seconds\": (boat_df['timestamp'].max() - boat_df['timestamp'].min()).total_seconds()\n",
    "}\n",
    "\n",
    "# Create timepoints with correct timestamps and values\n",
    "timepoints = []\n",
    "for _, row in boat_df.iterrows():\n",
    "    if pd.notna(row['final_latitude']) and pd.notna(row['final_longitude']):\n",
    "        timepoint = {\n",
    "            \"timestamp\": row['timestamp'].strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            \"position\": [float(row['final_longitude']), float(row['final_latitude'])],\n",
    "            \"speed\": float(row['speed']) if pd.notna(row['speed']) else 0.0,\n",
    "            \"direction\": float(row['direction']) if pd.notna(row['direction']) else 0.0,\n",
    "            \"distance_from_start\": float(row['distance_from_start']) if pd.notna(row['distance_from_start']) else 0.0\n",
    "        }\n",
    "        timepoints.append(timepoint)\n",
    "\n",
    "# Store boat data with corrected values\n",
    "race_data[\"boats\"][str(boat_id)] = {\n",
    "    \"track\": {\n",
    "        \"type\": \"LineString\",\n",
    "        \"coordinates\": coordinates\n",
    "    },\n",
    "    \"timepoints\": timepoints,\n",
    "    \"metadata\": stats\n",
    "}\n",
    "\n",
    "# Debug output\n",
    "print(\"\\nProcessed Data Summary:\")\n",
    "print(f\"Total boats: {len(race_data['boats'])}\")\n",
    "print(f\"Time range: {race_data['metadata']['time_range']}\")\n",
    "print(\"\\nSample timepoint:\")\n",
    "if timepoints:\n",
    "    print(json.dumps(timepoints[0], indent=2))\n",
    "print(\"\\nCoordinate bounds:\")\n",
    "print(json.dumps(race_data['metadata']['geographic_bounds'], indent=2))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
